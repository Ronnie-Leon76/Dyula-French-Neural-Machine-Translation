{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ES8iTKcdPCLt"
      },
      "source": [
        "# Subword tokenizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swymtxpl7W7w"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-25T11:38:17.874466Z",
          "iopub.status.busy": "2024-06-25T11:38:17.873857Z",
          "iopub.status.idle": "2024-06-25T11:38:43.021967Z",
          "shell.execute_reply": "2024-06-25T11:38:43.021130Z"
        },
        "id": "rJTYbk1E9QOk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b91de7d-cca8-463f-aacd-62715bf6029b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pandas-gbq 0.19.2 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\n",
            "tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 4.25.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q -U tensorflow-text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/koumankan_mt_dyu_fr_tfds.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZiyVx4dcDFX",
        "outputId": "ff725bfc-b20e-41de-bc79-6c8714cc541b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/koumankan_mt_dyu_fr_tfds.zip\n",
            "   creating: koumankan_mt_dyu_fr_tfds/\n",
            "  inflating: koumankan_mt_dyu_fr_tfds/koumankan_mt_dyu_fr_tfds_dataset_builder.py  \n",
            "  inflating: koumankan_mt_dyu_fr_tfds/koumankan_mt_dyu_fr_tfds_dataset_builder_test.py  \n",
            "  inflating: koumankan_mt_dyu_fr_tfds/README.md  \n",
            "  inflating: koumankan_mt_dyu_fr_tfds/CITATIONS.bib  \n",
            "  inflating: koumankan_mt_dyu_fr_tfds/TAGS.txt  \n",
            "  inflating: koumankan_mt_dyu_fr_tfds/__init__.py  \n",
            "  inflating: koumankan_mt_dyu_fr_tfds/checksums.tsv  \n",
            "   creating: koumankan_mt_dyu_fr_tfds/data/\n",
            "  inflating: koumankan_mt_dyu_fr_tfds/data/train_refined.csv  \n",
            "  inflating: koumankan_mt_dyu_fr_tfds/data/val_refined.csv  \n",
            "  inflating: koumankan_mt_dyu_fr_tfds/data/test_refined.csv  \n",
            "   creating: koumankan_mt_dyu_fr_tfds/dummy_data/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tfds-nightly tensorflow matplotlib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UW8uNTyebz7U",
        "outputId": "d9e31ac4-8708-4304-b660-805de15c9ca4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.3/309.3 kB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m102.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m95.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m100.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m104.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pandas-gbq 0.19.2 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\n",
            "tensorflow-text 2.11.0 requires tensorflow<2.12,>=2.11.0; platform_machine != \"arm64\" or platform_system != \"Darwin\", but you have tensorflow 2.16.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q apache-beam"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ghNDwYVcmHn",
        "outputId": "4d522601-5ae5-41f1-9ccc-63e705f738c1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.5/14.5 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.7/89.7 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.0/145.0 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.0/152.0 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m92.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.1/252.1 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for crcmod (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for hdfs (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyjsparser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q mlcroissant"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hi7fuIm4cxqA",
        "outputId": "64621bfc-eb55-42fa-aed0-43bc8e0b38be"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.3/126.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m531.9/531.9 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for jsonpath-rw (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tfds --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wn8hd5c8cbFv",
        "outputId": "47c0e072-010f-4940-c68f-ad193b2abc79"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow Datasets: 4.9.6+nightly\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/koumankan_mt_dyu_fr_tfds && tfds build"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Uk7j3S-c8dS",
        "outputId": "42cfc354-43eb-4291-88c2-0b85b02a33b2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO[build.py]: Loading dataset  from path: /content/koumankan_mt_dyu_fr_tfds/koumankan_mt_dyu_fr_tfds_dataset_builder.py\n",
            "2024-07-02 12:38:32.065218: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-02 12:38:32.090438: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-02 12:38:32.090490: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-02 12:38:32.106130: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-02 12:38:33.180767: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-02 12:38:33.850986: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-02 12:38:33.851284: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-02 12:38:33.942674: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-07-02 12:38:35.646884: W external/local_tsl/tsl/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"NOT_FOUND: Error executing an HTTP request: HTTP response code 404\".\n",
            "INFO[cli_utils.py]: download_and_prepare for dataset koumankan_mt_dyu_fr_tfds/1.0.0...\n",
            "INFO[dataset_builder.py]: Generating dataset koumankan_mt_dyu_fr_tfds (/root/tensorflow_datasets/koumankan_mt_dyu_fr_tfds/1.0.0)\n",
            "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to /root/tensorflow_datasets/koumankan_mt_dyu_fr_tfds/1.0.0...\u001b[0m\n",
            "Generating splits...:   0% 0/3 [00:00<?, ? splits/s]\n",
            "Generating train examples...: 0 examples [00:00, ? examples/s]\u001b[A\n",
            "Generating train examples...: 5776 examples [00:01, 5775.92 examples/s]\u001b[A\n",
            "                                                                       \u001b[A\n",
            "Shuffling /root/tensorflow_datasets/koumankan_mt_dyu_fr_tfds/incomplete.3ZH803_1.0.0/koumankan_mt_dyu_fr_tfds-train.tfrecord*...:   0% 0/8065 [00:00<?, ? examples/s]\u001b[A\n",
            "INFO[writer.py]: Done writing /root/tensorflow_datasets/koumankan_mt_dyu_fr_tfds/incomplete.3ZH803_1.0.0/koumankan_mt_dyu_fr_tfds-train.tfrecord*. Number of examples: 8065 (shards: [8065])\n",
            "Generating splits...:  33% 1/3 [00:01<00:02,  1.39s/ splits]\n",
            "Generating validation examples...: 0 examples [00:00, ? examples/s]\u001b[A\n",
            "                                                                   \u001b[A\n",
            "Shuffling /root/tensorflow_datasets/koumankan_mt_dyu_fr_tfds/incomplete.3ZH803_1.0.0/koumankan_mt_dyu_fr_tfds-validation.tfrecord*...:   0% 0/1471 [00:00<?, ? examples/s]\u001b[A\n",
            "INFO[writer.py]: Done writing /root/tensorflow_datasets/koumankan_mt_dyu_fr_tfds/incomplete.3ZH803_1.0.0/koumankan_mt_dyu_fr_tfds-validation.tfrecord*. Number of examples: 1471 (shards: [1471])\n",
            "Generating splits...:  67% 2/3 [00:01<00:00,  1.38 splits/s]\n",
            "Generating test examples...: 0 examples [00:00, ? examples/s]\u001b[A\n",
            "                                                             \u001b[A\n",
            "Shuffling /root/tensorflow_datasets/koumankan_mt_dyu_fr_tfds/incomplete.3ZH803_1.0.0/koumankan_mt_dyu_fr_tfds-test.tfrecord*...:   0% 0/1393 [00:00<?, ? examples/s]\u001b[A\n",
            "INFO[writer.py]: Done writing /root/tensorflow_datasets/koumankan_mt_dyu_fr_tfds/incomplete.3ZH803_1.0.0/koumankan_mt_dyu_fr_tfds-test.tfrecord*. Number of examples: 1393 (shards: [1393])\n",
            "\u001b[1mDataset koumankan_mt_dyu_fr_tfds downloaded and prepared to /root/tensorflow_datasets/koumankan_mt_dyu_fr_tfds/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n",
            "INFO[cli_utils.py]: Dataset generation complete...\n",
            "\n",
            "tfds.core.DatasetInfo(\n",
            "    name='koumankan_mt_dyu_fr_tfds',\n",
            "    full_name='koumankan_mt_dyu_fr_tfds/1.0.0',\n",
            "    description=\"\"\"\n",
            "    This dataset contains translations from Dyula to French.\n",
            "    \"\"\",\n",
            "    homepage='https://huggingface.co/datasets/uvci/Koumankan_mt_dyu_fr',\n",
            "    data_dir='/root/tensorflow_datasets/koumankan_mt_dyu_fr_tfds/1.0.0',\n",
            "    file_format=tfrecord,\n",
            "    download_size=Unknown size,\n",
            "    dataset_size=906.15 KiB,\n",
            "    features=FeaturesDict({\n",
            "        'dyu': Text(shape=(), dtype=string),\n",
            "        'fr': Text(shape=(), dtype=string),\n",
            "    }),\n",
            "    supervised_keys=('dyu', 'fr'),\n",
            "    disable_shuffling=False,\n",
            "    splits={\n",
            "        'test': <SplitInfo num_examples=1393, num_shards=1>,\n",
            "        'train': <SplitInfo num_examples=8065, num_shards=1>,\n",
            "        'validation': <SplitInfo num_examples=1471, num_shards=1>,\n",
            "    },\n",
            "    citation=\"\"\"\"\"\",\n",
            ")\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-25T11:38:43.026705Z",
          "iopub.status.busy": "2024-06-25T11:38:43.025967Z",
          "iopub.status.idle": "2024-06-25T11:38:44.930144Z",
          "shell.execute_reply": "2024-06-25T11:38:44.929250Z"
        },
        "id": "XFG0NDRu5mYQ"
      },
      "outputs": [],
      "source": [
        "!pip install -q tensorflow_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-25T11:38:44.934437Z",
          "iopub.status.busy": "2024-06-25T11:38:44.934140Z",
          "iopub.status.idle": "2024-06-25T11:38:47.775046Z",
          "shell.execute_reply": "2024-06-25T11:38:47.774338Z"
        },
        "id": "JjJJyJTZYebt"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import os\n",
        "import pathlib\n",
        "import re\n",
        "import string\n",
        "import sys\n",
        "import tempfile\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_text as text\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-25T11:38:47.779200Z",
          "iopub.status.busy": "2024-06-25T11:38:47.778760Z",
          "iopub.status.idle": "2024-06-25T11:38:47.782545Z",
          "shell.execute_reply": "2024-06-25T11:38:47.781967Z"
        },
        "id": "QZi9RstHxO_Z"
      },
      "outputs": [],
      "source": [
        "tf.get_logger().setLevel('ERROR')\n",
        "pwd = pathlib.Path.cwd()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzJbGA5N5mXr"
      },
      "source": [
        "## Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-25T11:38:47.785749Z",
          "iopub.status.busy": "2024-06-25T11:38:47.785516Z",
          "iopub.status.idle": "2024-06-25T11:38:49.176050Z",
          "shell.execute_reply": "2024-06-25T11:38:49.175342Z"
        },
        "id": "qDaAOTKHNy8e"
      },
      "outputs": [],
      "source": [
        "examples, metadata = tfds.load('koumankan_mt_dyu_fr_tfds', with_info=True,\n",
        "                               as_supervised=True)\n",
        "train_examples, val_examples = examples['train'], examples['validation']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GHc3O2W8Hgg"
      },
      "source": [
        "This dataset produces Dyula/French sentence pairs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-25T11:38:49.179991Z",
          "iopub.status.busy": "2024-06-25T11:38:49.179473Z",
          "iopub.status.idle": "2024-06-25T11:38:49.595381Z",
          "shell.execute_reply": "2024-06-25T11:38:49.594429Z"
        },
        "id": "-_ezZT8w8GqD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dd03b0c-2df0-4943-fce6-aacde3809e5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dyula:  ayi accident kè a ma mè\n",
            "French:    elle a eu un accident de la route la semaine dernière\n",
            "Dyula:  gèma alé ni kefé ni nonnon\n",
            "French:    blanc comme du café au lait\n"
          ]
        }
      ],
      "source": [
        "for dyu, fr in train_examples.take(2):\n",
        "  print(\"Dyula: \", dyu.numpy().decode('utf-8'))\n",
        "  print(\"French:   \", fr.numpy().decode('utf-8'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNGwm45vKttj"
      },
      "source": [
        "Note a few things about the example sentences above:\n",
        "* They're lower case.\n",
        "* There are spaces around the punctuation.\n",
        "* It's not clear if or what unicode normalization is being used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-25T11:38:49.599445Z",
          "iopub.status.busy": "2024-06-25T11:38:49.598757Z",
          "iopub.status.idle": "2024-06-25T11:38:49.629136Z",
          "shell.execute_reply": "2024-06-25T11:38:49.628547Z"
        },
        "id": "Pm5Eah5F6B1I"
      },
      "outputs": [],
      "source": [
        "train_fr = train_examples.map(lambda dyu, fr: fr)\n",
        "train_dyu = train_examples.map(lambda dyu, fr: dyu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCD57yALsF0D"
      },
      "source": [
        "## Generate the vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-25T11:38:49.633157Z",
          "iopub.status.busy": "2024-06-25T11:38:49.632577Z",
          "iopub.status.idle": "2024-06-25T11:38:49.636897Z",
          "shell.execute_reply": "2024-06-25T11:38:49.636354Z"
        },
        "id": "iqX1fYdpnLS2"
      },
      "outputs": [],
      "source": [
        "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaWSnj8xFgI7"
      },
      "source": [
        "The `bert_vocab.bert_vocab_from_dataset` function will generate the vocabulary.\n",
        "\n",
        "There are many arguments you can set to adjust its behavior. For this tutorial, you'll mostly use the defaults. If you want to learn more about the options, first read about [the algorithm](#algorithm), and then have a look at [the code](https://github.com/tensorflow/text/blob/master/tensorflow_text/tools/wordpiece_vocab/bert_vocab_from_dataset.py).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-25T11:38:49.640534Z",
          "iopub.status.busy": "2024-06-25T11:38:49.639914Z",
          "iopub.status.idle": "2024-06-25T11:38:49.643863Z",
          "shell.execute_reply": "2024-06-25T11:38:49.643248Z"
        },
        "id": "FwFzYjBy-h8W"
      },
      "outputs": [],
      "source": [
        "bert_tokenizer_params=dict(lower_case=True)\n",
        "reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
        "\n",
        "bert_vocab_args = dict(\n",
        "    # The target vocabulary size\n",
        "    vocab_size = 8000,\n",
        "    # Reserved tokens that must be included in the vocabulary\n",
        "    reserved_tokens=reserved_tokens,\n",
        "    # Arguments for `text.BertTokenizer`\n",
        "    bert_tokenizer_params=bert_tokenizer_params,\n",
        "    # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n",
        "    learn_params={},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-25T11:38:49.647016Z",
          "iopub.status.busy": "2024-06-25T11:38:49.646408Z",
          "iopub.status.idle": "2024-06-25T11:40:09.428441Z",
          "shell.execute_reply": "2024-06-25T11:40:09.427374Z"
        },
        "id": "PMN6Lli_3sJW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "338d98c9-1a33-4d6e-ba61-ac3e7489fd47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 10.7 s, sys: 269 ms, total: 11 s\n",
            "Wall time: 10.8 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "dyu_vocab = bert_vocab.bert_vocab_from_dataset(\n",
        "    train_dyu.batch(1000).prefetch(2),\n",
        "    **bert_vocab_args\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Cl4d2O34gkH"
      },
      "source": [
        "Here are some slices of the resulting vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-25T11:40:09.432497Z",
          "iopub.status.busy": "2024-06-25T11:40:09.431791Z",
          "iopub.status.idle": "2024-06-25T11:40:09.436846Z",
          "shell.execute_reply": "2024-06-25T11:40:09.435890Z"
        },
        "id": "mfaPmX54FvhW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "166fc347-83f9-4131-a02e-00bcf5ed2fe0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[PAD]', '[UNK]', '[START]', '[END]', \"'\", '0', '1', 'a', 'b', 'c']\n",
            "['##man', 'kalan', '##o', 'bara', '##t', 'kuma', 'yan', 'taga', 'nunu', 'ya']\n",
            "[]\n",
            "['ɲanama', 'ɲɛn', \"##'\", '##0', '##1', '##j', '##q', '##v', '##x', '##ŋ']\n"
          ]
        }
      ],
      "source": [
        "print(dyu_vocab[:10])\n",
        "print(dyu_vocab[100:110])\n",
        "print(dyu_vocab[1000:1010])\n",
        "print(dyu_vocab[-10:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owkP3wbYVQv0"
      },
      "source": [
        "Write a vocabulary file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-25T11:40:09.440290Z",
          "iopub.status.busy": "2024-06-25T11:40:09.439825Z",
          "iopub.status.idle": "2024-06-25T11:40:09.443816Z",
          "shell.execute_reply": "2024-06-25T11:40:09.442990Z"
        },
        "id": "VY6v1ThkKDyZ"
      },
      "outputs": [],
      "source": [
        "def write_vocab_file(filepath, vocab):\n",
        "  with open(filepath, 'w') as f:\n",
        "    for token in vocab:\n",
        "      print(token, file=f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-25T11:40:09.447156Z",
          "iopub.status.busy": "2024-06-25T11:40:09.446331Z",
          "iopub.status.idle": "2024-06-25T11:40:09.453497Z",
          "shell.execute_reply": "2024-06-25T11:40:09.452672Z"
        },
        "id": "X_TR5U1xWvAV"
      },
      "outputs": [],
      "source": [
        "write_vocab_file('dyu_vocab.txt', dyu_vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ag3qcx54nii"
      },
      "source": [
        "Use that function to generate a vocabulary from the french data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-25T11:40:09.456627Z",
          "iopub.status.busy": "2024-06-25T11:40:09.456352Z",
          "iopub.status.idle": "2024-06-25T11:41:04.243609Z",
          "shell.execute_reply": "2024-06-25T11:41:04.242825Z"
        },
        "id": "R3cMumvHWWtl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16d68f16-73e7-494e-eb5f-09f1c5e5df79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 15.8 s, sys: 146 ms, total: 15.9 s\n",
            "Wall time: 16.7 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "fr_vocab = bert_vocab.bert_vocab_from_dataset(\n",
        "    train_fr.batch(1000).prefetch(2),\n",
        "    **bert_vocab_args\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-25T11:41:04.247394Z",
          "iopub.status.busy": "2024-06-25T11:41:04.246673Z",
          "iopub.status.idle": "2024-06-25T11:41:04.251013Z",
          "shell.execute_reply": "2024-06-25T11:41:04.250378Z"
        },
        "id": "NxOpzMd8ol5B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d093e582-f409-4acc-d235-ea97994ba410"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[PAD]', '[UNK]', '[START]', '[END]', \"'\", '.', 'a', 'b', 'c', 'd']\n",
            "['par', '##on', '##n', '##te', 'avez', 'avec', 'lui', 'trois', 'sa', 'moi']\n",
            "['attendre', 'aurais', 'autrefois', 'avaient', 'besoin', 'breton', 'but', 'cafe', 'capitaine', 'danser']\n",
            "['##ł', '##œ', '##ʻ', '##α', '##–', '##—', '##’', '##“', '##”', '##€']\n"
          ]
        }
      ],
      "source": [
        "print(fr_vocab[:10])\n",
        "print(fr_vocab[100:110])\n",
        "print(fr_vocab[1000:1010])\n",
        "print(fr_vocab[-10:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ck3LG_f34wCs"
      },
      "source": [
        "Here are the two vocabulary files:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-25T11:41:04.254315Z",
          "iopub.status.busy": "2024-06-25T11:41:04.253904Z",
          "iopub.status.idle": "2024-06-25T11:41:04.259819Z",
          "shell.execute_reply": "2024-06-25T11:41:04.259224Z"
        },
        "id": "xfc2jxPznM6H"
      },
      "outputs": [],
      "source": [
        "write_vocab_file('fr_vocab.txt', fr_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-25T11:41:04.262983Z",
          "iopub.status.busy": "2024-06-25T11:41:04.262456Z",
          "iopub.status.idle": "2024-06-25T11:41:04.401328Z",
          "shell.execute_reply": "2024-06-25T11:41:04.400428Z"
        },
        "id": "djehfEL6Zn-I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dc84859-f31f-45e2-f040-a61e10fd34e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dyu_vocab.txt  fr_vocab.txt\n"
          ]
        }
      ],
      "source": [
        "!ls *.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vb5ddYLTBJhk"
      },
      "source": [
        "## Build the tokenizer\n",
        "<a id=\"build_the_tokenizer\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qgp5gvR-2tQ"
      },
      "source": [
        "The `text.BertTokenizer` can be initialized by passing the vocabulary file's path as the first argument (see the section on [tf.lookup](#tf.lookup) for other options):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-25T11:41:04.405593Z",
          "iopub.status.busy": "2024-06-25T11:41:04.404932Z",
          "iopub.status.idle": "2024-06-25T11:41:04.419115Z",
          "shell.execute_reply": "2024-06-25T11:41:04.418540Z"
        },
        "id": "gdMpt9ZEjVGu"
      },
      "outputs": [],
      "source": [
        "dyu_tokenizer = text.BertTokenizer('dyu_vocab.txt', **bert_tokenizer_params)\n",
        "fr_tokenizer = text.BertTokenizer('fr_vocab.txt', **bert_tokenizer_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhPZafCUds86"
      },
      "source": [
        "Now you can use it to encode some text. Take a batch of 3 examples from the english data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-25T11:41:04.422690Z",
          "iopub.status.busy": "2024-06-25T11:41:04.422231Z",
          "iopub.status.idle": "2024-06-25T11:41:04.651767Z",
          "shell.execute_reply": "2024-06-25T11:41:04.650941Z"
        },
        "id": "NKF0QJjtUm9T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebeb90a2-7b05-4e85-d99d-6032431cc837"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'elle a eu un accident de la route la semaine derni\\xc3\\xa8re'\n",
            "b'blanc comme du caf\\xc3\\xa9 au lait'\n",
            "b'la baffe  \\xc3\\xa7a humilie'\n"
          ]
        }
      ],
      "source": [
        "for dyu_examples, fr_examples in train_examples.batch(3).take(1):\n",
        "  for ex in fr_examples:\n",
        "    print(ex.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9OEIBWopMxW"
      },
      "source": [
        "Run it through the `BertTokenizer.tokenize` method. Initially, this returns a `tf.RaggedTensor` with axes `(batch, word, word-piece)`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-25T11:41:04.655585Z",
          "iopub.status.busy": "2024-06-25T11:41:04.654848Z",
          "iopub.status.idle": "2024-06-25T11:41:04.697887Z",
          "shell.execute_reply": "2024-06-25T11:41:04.697290Z"
        },
        "id": "AeTM81lAc8q1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9add36ad-593a-40a5-95b5-fd312b23e814"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[62, 6, 454, 52, 6, 130, 130, 959, 44, 46, 132, 46, 76, 499, 161, 528, 54]\n",
            "[7, 628, 130, 144, 61, 1007, 73, 1025]\n",
            "[46, 7, 89, 338, 952, 80, 13, 450, 254, 121]\n"
          ]
        }
      ],
      "source": [
        "# Tokenize the examples -> (batch, word, word-piece)\n",
        "token_batch = fr_tokenizer.tokenize(fr_examples)\n",
        "# Merge the word and word-piece axes -> (batch, tokens)\n",
        "token_batch = token_batch.merge_dims(-2,-1)\n",
        "\n",
        "for ex in token_batch.to_list():\n",
        "  print(ex)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-25T11:41:04.701301Z",
          "iopub.status.busy": "2024-06-25T11:41:04.701040Z",
          "iopub.status.idle": "2024-06-25T11:41:04.740575Z",
          "shell.execute_reply": "2024-06-25T11:41:04.739925Z"
        },
        "id": "FA6nKYx5U3Nj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40fc480d-36cc-4ccc-b53c-db28bd083b0f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3,), dtype=string, numpy=\n",
              "array([b'elle a eu un a ##c ##c ##ident de la route la se ##ma ##ine dernier ##e',\n",
              "       b'b ##lan ##c comme du cafe au lait',\n",
              "       b'la b ##a ##f ##fe ca h ##um ##il ##ie'], dtype=object)>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "# Lookup each token id in the vocabulary.\n",
        "txt_tokens = tf.gather(fr_vocab, token_batch)\n",
        "# Join with spaces.\n",
        "tf.strings.reduce_join(txt_tokens, separator=' ', axis=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wY2XrhyRem2O"
      },
      "source": [
        "To re-assemble words from the extracted tokens, use the `BertTokenizer.detokenize` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-25T11:41:04.743932Z",
          "iopub.status.busy": "2024-06-25T11:41:04.743407Z",
          "iopub.status.idle": "2024-06-25T11:41:04.783773Z",
          "shell.execute_reply": "2024-06-25T11:41:04.783141Z"
        },
        "id": "toBXQSrgemRw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df2396da-45d2-4a30-b11b-fcbf56edfc19"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3,), dtype=string, numpy=\n",
              "array([b'elle a eu un accident de la route la semaine derniere',\n",
              "       b'blanc comme du cafe au lait', b'la baffe ca humilie'],\n",
              "      dtype=object)>"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "words = fr_tokenizer.detokenize(token_batch)\n",
        "tf.strings.reduce_join(words, separator=' ', axis=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIZWWy_iueQY"
      },
      "source": [
        "> Note: `BertTokenizer.tokenize`/`BertTokenizer.detokenize` does not round\n",
        "trip losslessly. The result of `detokenize` will not, in general, have the\n",
        "same content or offsets as the input to `tokenize`. This is because of the\n",
        "\"basic tokenization\" step, that splits the strings into words before\n",
        "applying the `WordpieceTokenizer`, includes irreversible\n",
        "steps like lower-casing and splitting on punctuation. `WordpieceTokenizer`\n",
        "on the other hand **is** reversible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wpc7oFkwgni"
      },
      "source": [
        "### Custom tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaUR9hHj0PUy"
      },
      "source": [
        "The downstream tutorials both expect the tokenized text to include `[START]` and `[END]` tokens.\n",
        "\n",
        "The `reserved_tokens` reserve space at the beginning of the vocabulary, so `[START]` and `[END]` have the same indexes for both languages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-25T11:41:04.787367Z",
          "iopub.status.busy": "2024-06-25T11:41:04.786866Z",
          "iopub.status.idle": "2024-06-25T11:41:04.793672Z",
          "shell.execute_reply": "2024-06-25T11:41:04.793103Z"
        },
        "id": "gyyoa5De0WQu"
      },
      "outputs": [],
      "source": [
        "START = tf.argmax(tf.constant(reserved_tokens) == \"[START]\")\n",
        "END = tf.argmax(tf.constant(reserved_tokens) == \"[END]\")\n",
        "\n",
        "def add_start_end(ragged):\n",
        "  count = ragged.bounding_shape()[0]\n",
        "  starts = tf.fill([count,1], START)\n",
        "  ends = tf.fill([count,1], END)\n",
        "  return tf.concat([starts, ragged, ends], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-25T11:41:04.796763Z",
          "iopub.status.busy": "2024-06-25T11:41:04.796411Z",
          "iopub.status.idle": "2024-06-25T11:41:04.834141Z",
          "shell.execute_reply": "2024-06-25T11:41:04.833568Z"
        },
        "id": "MrZjQIwZ6NHu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af7c0b98-8665-490b-91a7-33e7e2d600c2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3,), dtype=string, numpy=\n",
              "array([b'[START] elle a eu un accident de la route la semaine derniere [END]',\n",
              "       b'[START] blanc comme du cafe au lait [END]',\n",
              "       b'[START] la baffe ca humilie [END]'], dtype=object)>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "words = fr_tokenizer.detokenize(add_start_end(token_batch))\n",
        "tf.strings.reduce_join(words, separator=' ', axis=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMmHS5VT_suH"
      },
      "source": [
        "### Custom detokenization\n",
        "\n",
        "Before exporting the tokenizers there are a couple of things you can cleanup for the downstream tutorials:\n",
        "\n",
        "1. They want to generate clean text output, so drop reserved tokens like `[START]`, `[END]` and `[PAD]`.\n",
        "2. They're interested in complete strings, so apply a string join along the `words` axis of the result.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-25T11:41:04.837629Z",
          "iopub.status.busy": "2024-06-25T11:41:04.837058Z",
          "iopub.status.idle": "2024-06-25T11:41:04.841342Z",
          "shell.execute_reply": "2024-06-25T11:41:04.840711Z"
        },
        "id": "x9vXUQPX1ZFA"
      },
      "outputs": [],
      "source": [
        "def cleanup_text(reserved_tokens, token_txt):\n",
        "  # Drop the reserved tokens, except for \"[UNK]\".\n",
        "  bad_tokens = [re.escape(tok) for tok in reserved_tokens if tok != \"[UNK]\"]\n",
        "  bad_token_re = \"|\".join(bad_tokens)\n",
        "\n",
        "  bad_cells = tf.strings.regex_full_match(token_txt, bad_token_re)\n",
        "  result = tf.ragged.boolean_mask(token_txt, ~bad_cells)\n",
        "\n",
        "  # Join them into strings.\n",
        "  result = tf.strings.reduce_join(result, separator=' ', axis=-1)\n",
        "\n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-25T11:41:04.844525Z",
          "iopub.status.busy": "2024-06-25T11:41:04.844016Z",
          "iopub.status.idle": "2024-06-25T11:41:04.848264Z",
          "shell.execute_reply": "2024-06-25T11:41:04.847681Z"
        },
        "id": "NMSpZUV7sQYw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20ea53a1-1a55-46ad-91e4-99c001522f55"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'elle a eu un accident de la route la semaine derni\\xc3\\xa8re',\n",
              "       b'blanc comme du caf\\xc3\\xa9 au lait',\n",
              "       b'la baffe  \\xc3\\xa7a humilie'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "fr_examples.numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-25T11:41:04.851459Z",
          "iopub.status.busy": "2024-06-25T11:41:04.850943Z",
          "iopub.status.idle": "2024-06-25T11:41:04.877021Z",
          "shell.execute_reply": "2024-06-25T11:41:04.876460Z"
        },
        "id": "yB3MJhNvkuBb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6adb225c-48fc-409c-e781-c065c030fa92"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'elle', b'a', b'eu', b'un', b'accident', b'de', b'la', b'route', b'la',\n",
              "  b'semaine', b'derniere']                                                ,\n",
              " [b'blanc', b'comme', b'du', b'cafe', b'au', b'lait'],\n",
              " [b'la', b'baffe', b'ca', b'humilie']]>"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "token_batch = fr_tokenizer.tokenize(fr_examples).merge_dims(-2,-1)\n",
        "words = fr_tokenizer.detokenize(token_batch)\n",
        "words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-25T11:41:04.880255Z",
          "iopub.status.busy": "2024-06-25T11:41:04.879766Z",
          "iopub.status.idle": "2024-06-25T11:41:04.898364Z",
          "shell.execute_reply": "2024-06-25T11:41:04.897752Z"
        },
        "id": "ED5rMeZE6HT3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30336b3b-66dc-47da-e076-9f79b57dd430"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'elle a eu un accident de la route la semaine derniere',\n",
              "       b'blanc comme du cafe au lait', b'la baffe ca humilie'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "cleanup_text(reserved_tokens, words).numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEfEdRi11Re4"
      },
      "source": [
        "### Export"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFuo1KZjpEPR"
      },
      "source": [
        "The following code block builds a `CustomTokenizer` class to contain the `text.BertTokenizer` instances, the custom logic, and the `@tf.function` wrappers required for export."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-25T11:41:04.901932Z",
          "iopub.status.busy": "2024-06-25T11:41:04.901493Z",
          "iopub.status.idle": "2024-06-25T11:41:04.911306Z",
          "shell.execute_reply": "2024-06-25T11:41:04.910654Z"
        },
        "id": "f1q1hCpH72Vj"
      },
      "outputs": [],
      "source": [
        "class CustomTokenizer(tf.Module):\n",
        "  def __init__(self, reserved_tokens, vocab_path):\n",
        "    self.tokenizer = text.BertTokenizer(vocab_path, lower_case=True)\n",
        "    self._reserved_tokens = reserved_tokens\n",
        "    self._vocab_path = tf.saved_model.Asset(vocab_path)\n",
        "\n",
        "    vocab = pathlib.Path(vocab_path).read_text().splitlines()\n",
        "    self.vocab = tf.Variable(vocab)\n",
        "\n",
        "    ## Create the signatures for export:\n",
        "\n",
        "    # Include a tokenize signature for a batch of strings.\n",
        "    self.tokenize.get_concrete_function(\n",
        "        tf.TensorSpec(shape=[None], dtype=tf.string))\n",
        "\n",
        "    # Include `detokenize` and `lookup` signatures for:\n",
        "    #   * `Tensors` with shapes [tokens] and [batch, tokens]\n",
        "    #   * `RaggedTensors` with shape [batch, tokens]\n",
        "    self.detokenize.get_concrete_function(\n",
        "        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "    self.detokenize.get_concrete_function(\n",
        "          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "\n",
        "    self.lookup.get_concrete_function(\n",
        "        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "    self.lookup.get_concrete_function(\n",
        "          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "\n",
        "    # These `get_*` methods take no arguments\n",
        "    self.get_vocab_size.get_concrete_function()\n",
        "    self.get_vocab_path.get_concrete_function()\n",
        "    self.get_reserved_tokens.get_concrete_function()\n",
        "\n",
        "  @tf.function\n",
        "  def tokenize(self, strings):\n",
        "    enc = self.tokenizer.tokenize(strings)\n",
        "    # Merge the `word` and `word-piece` axes.\n",
        "    enc = enc.merge_dims(-2,-1)\n",
        "    enc = add_start_end(enc)\n",
        "    return enc\n",
        "\n",
        "  @tf.function\n",
        "  def detokenize(self, tokenized):\n",
        "    words = self.tokenizer.detokenize(tokenized)\n",
        "    return cleanup_text(self._reserved_tokens, words)\n",
        "\n",
        "  @tf.function\n",
        "  def lookup(self, token_ids):\n",
        "    return tf.gather(self.vocab, token_ids)\n",
        "\n",
        "  @tf.function\n",
        "  def get_vocab_size(self):\n",
        "    return tf.shape(self.vocab)[0]\n",
        "\n",
        "  @tf.function\n",
        "  def get_vocab_path(self):\n",
        "    return self._vocab_path\n",
        "\n",
        "  @tf.function\n",
        "  def get_reserved_tokens(self):\n",
        "    return tf.constant(self._reserved_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHzEnTQM6nBD"
      },
      "source": [
        "Build a `CustomTokenizer` for each language:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-25T11:41:04.914786Z",
          "iopub.status.busy": "2024-06-25T11:41:04.914215Z",
          "iopub.status.idle": "2024-06-25T11:41:07.282524Z",
          "shell.execute_reply": "2024-06-25T11:41:07.281790Z"
        },
        "id": "cU8yFBCSruz4"
      },
      "outputs": [],
      "source": [
        "tokenizers = tf.Module()\n",
        "tokenizers.dyu = CustomTokenizer(reserved_tokens, 'dyu_vocab.txt')\n",
        "tokenizers.fr = CustomTokenizer(reserved_tokens, 'fr_vocab.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYfrmDhy6syT"
      },
      "source": [
        "Export the tokenizers as a `saved_model`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-25T11:41:07.286945Z",
          "iopub.status.busy": "2024-06-25T11:41:07.286338Z",
          "iopub.status.idle": "2024-06-25T11:41:09.561161Z",
          "shell.execute_reply": "2024-06-25T11:41:09.560356Z"
        },
        "id": "aieDGooa9ms7"
      },
      "outputs": [],
      "source": [
        "model_name = 'translate_dyula_french'\n",
        "tf.saved_model.save(tokenizers, model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoCMz2Fm61v6"
      },
      "source": [
        "Reload the `saved_model` and test the methods:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-25T11:41:09.565879Z",
          "iopub.status.busy": "2024-06-25T11:41:09.565362Z",
          "iopub.status.idle": "2024-06-25T11:41:10.251887Z",
          "shell.execute_reply": "2024-06-25T11:41:10.251059Z"
        },
        "id": "9SB_BHwqsHkb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "835e543e-5f6a-47a7-c1a2-21f2b178477f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1069"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "reloaded_tokenizers = tf.saved_model.load(model_name)\n",
        "reloaded_tokenizers.fr.get_vocab_size().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-25T11:41:10.255313Z",
          "iopub.status.busy": "2024-06-25T11:41:10.255006Z",
          "iopub.status.idle": "2024-06-25T11:41:10.488969Z",
          "shell.execute_reply": "2024-06-25T11:41:10.488345Z"
        },
        "id": "W_Ze3WL3816x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7eb23f0-83d2-469d-8cd3-14601690357e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   2,  243, 1054,  340,  251,  377,  410,  338,  967, 1056,    1,\n",
              "           3]])"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "tokens = reloaded_tokenizers.fr.tokenize(['Bonjour TensorFlow!'])\n",
        "tokens.numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-25T11:41:10.492442Z",
          "iopub.status.busy": "2024-06-25T11:41:10.492011Z",
          "iopub.status.idle": "2024-06-25T11:41:10.512373Z",
          "shell.execute_reply": "2024-06-25T11:41:10.511722Z"
        },
        "id": "v9o93bzcuhyC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38426459-7b85-46d8-9991-53e0dadd7313"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'[START]', b'bon', b'##j', b'##our', b'te', b'##ns', b'##or', b'##f',\n",
              "  b'##lo', b'##w', b'[UNK]', b'[END]']]>"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "text_tokens = reloaded_tokenizers.fr.lookup(tokens)\n",
        "text_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-25T11:41:10.515480Z",
          "iopub.status.busy": "2024-06-25T11:41:10.515225Z",
          "iopub.status.idle": "2024-06-25T11:41:10.627851Z",
          "shell.execute_reply": "2024-06-25T11:41:10.627146Z"
        },
        "id": "Y0205N_8dDT5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12baaa4f-fb85-45bd-e2a2-29032ae1f029"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bonjour tensorflow [UNK]\n"
          ]
        }
      ],
      "source": [
        "round_trip = reloaded_tokenizers.fr.detokenize(tokens)\n",
        "\n",
        "print(round_trip.numpy()[0].decode('utf-8'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSKFDQoBjnNp"
      },
      "source": [
        "Archive it for the [translation tutorials](https://tensorflow.org/text/tutorials/transformer):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-25T11:41:10.631588Z",
          "iopub.status.busy": "2024-06-25T11:41:10.630867Z",
          "iopub.status.idle": "2024-06-25T11:41:10.806819Z",
          "shell.execute_reply": "2024-06-25T11:41:10.805985Z"
        },
        "id": "eY0SoE3Yj2it",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ba51464-8cea-4d44-b313-b9944ab86ec4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: translate_dyula_french/ (stored 0%)\n",
            "  adding: translate_dyula_french/assets/ (stored 0%)\n",
            "  adding: translate_dyula_french/assets/fr_vocab.txt (deflated 53%)\n",
            "  adding: translate_dyula_french/assets/dyu_vocab.txt (deflated 52%)\n",
            "  adding: translate_dyula_french/saved_model.pb (deflated 91%)\n",
            "  adding: translate_dyula_french/fingerprint.pb (stored 0%)\n",
            "  adding: translate_dyula_french/variables/ (stored 0%)\n",
            "  adding: translate_dyula_french/variables/variables.data-00000-of-00001 (deflated 49%)\n",
            "  adding: translate_dyula_french/variables/variables.index (deflated 33%)\n"
          ]
        }
      ],
      "source": [
        "!zip -r {model_name}.zip {model_name}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-25T11:41:10.810738Z",
          "iopub.status.busy": "2024-06-25T11:41:10.810124Z",
          "iopub.status.idle": "2024-06-25T11:41:10.949466Z",
          "shell.execute_reply": "2024-06-25T11:41:10.948689Z"
        },
        "id": "0Synq0RekAXe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a168ddbc-2ef4-497a-e2f0-82ad0f5807e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "272K\tkoumankan_mt_dyu_fr_tfds.zip\n",
            "80K\ttranslate_dyula_french.zip\n"
          ]
        }
      ],
      "source": [
        "!du -h *.zip"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}